{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_YoxRBfhCACv",
        "LPCKiTHvHYUa",
        "6PYvMBACJ4PD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrigoreGeorgeAlexandru/Colab-projects/blob/main/Lab_2_APD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0i9IFlJZB7DW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmi Paraleli si Distribuiti - Lab 2"
      ],
      "metadata": {
        "id": "H422e-DYCB1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Barrier.\n",
        "\n",
        "Scrieti un program paralel care sa creeze mai multe procese care se sincronizeaza folosind bariere in felul urmator:\n",
        "* procesul zero (`root`) simuleaza o activitate dormind 4 secunde dupa care scrie un mesaj pe ecran;\n",
        "* toate celelalte procesoare asteapta sa termine procesorul zero folosind o bariera, apoi scriu un mesaj pe ecran avertizand ca se pregatesc de lucru (de fapt se pregatesc de `sleep`)\n",
        "* apoi, toate procesoarele simuleaza o activitate aleatoare intre 0 si 10 secunde folosind `sleep`, iar dupa terminarea activitatii scriu un mesaj pe ecran\n",
        "\n",
        "Apoi, toate procesoarele apeleaza rutina `Barrier` pentru a se sincroniza unele cu altele.\n",
        "\n",
        "La final, dupa deblocarea din bariera, procesul `root` conclude ca toate procesele s-au terminat si scrie un mesaj pe ecran in acest sens."
      ],
      "metadata": {
        "id": "_YoxRBfhCACv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importurile\n",
        "import multiprocessing\n",
        "import time\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "2ouBOZ5xB45J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_task(pid, barrier1, barrier2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        pid (int): Numarul procesului - 0 e `root`.\n",
        "        barrier1 (multiprocessing.Barrier): Prima bariera.\n",
        "        barrier2 (multiprocessing.Barrier): A 2-a bariera.\n",
        "    \"\"\"\n",
        "    # Simululez procesului 0\n",
        "    if pid == 0:\n",
        "        print(\"Incepe procesul 0 (`root`). Incep un task care dureaza 4 secunde.\")\n",
        "        time.sleep(4)\n",
        "        print(\"Procesul 0 (`root`) a terminat job-ul de 4 secunde.\")\n",
        "\n",
        "    # Astept la prima bariera.\n",
        "    barrier1.wait()\n",
        "\n",
        "    # Fac un print sa anunte ca au terminat de asteptat\n",
        "    if pid != 0:\n",
        "        print(f\"Procesul {pid} a terminat de asteptat.\")\n",
        "\n",
        "    # Generez un timp random cat sa dureze fiecare proces (fac sleep).\n",
        "    sleep_duration = random.uniform(0, 10)\n",
        "    print(f\"Procesul {pid} va dura {sleep_duration:.2f} secunde.\")\n",
        "    time.sleep(sleep_duration)\n",
        "    print(f\"Procesul {pid} si-a terminat treaba.\")\n",
        "\n",
        "    # Folosesc o alta bariera ca sa sincronizez toate procesele\n",
        "    print(f\"Procesul {pid} asteapta la a 2-a bariera.\")\n",
        "    barrier2.wait()\n",
        "\n",
        "    # Daca sunt procesul 0, atunci afisez mesajul dorit\n",
        "    if pid == 0:\n",
        "        print(\"========================================================\")\n",
        "        print(\"===    Procesul 0: Toate procesele s-au terminat.    ===\")\n",
        "        print(\"========================================================\")\n"
      ],
      "metadata": {
        "id": "FAGGT7LnDU7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definesc numarul de procese\n",
        "num_procese = 5  # 1 (root) + 4 = 5\n",
        "\n",
        "print(f\"Am inceput. Numarul de procese = {num_procese}.\\n\")\n",
        "\n",
        "barrier1 = multiprocessing.Barrier(num_procese)\n",
        "barrier2 = multiprocessing.Barrier(num_procese)\n",
        "\n",
        "# Fac o lista de procese ca dupa sa ii fac join.\n",
        "processes = []\n",
        "\n",
        "# Creez fiecare dintre cele 5 procese.\n",
        "for i in range(num_procese):\n",
        "    process = multiprocessing.Process(\n",
        "        target=process_task,\n",
        "        args=(i, barrier1, barrier2)\n",
        "    )\n",
        "    processes.append(process)\n",
        "    process.start()  # Apelez `process_task` pentru procesul process\n",
        "\n",
        "for process in processes:\n",
        "    process.join()"
      ],
      "metadata": {
        "id": "AbxW9oDqFaGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89b7f16-52d4-4348-e6d8-df913243cb72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Am inceput. Numarul de procese = 5.\n",
            "\n",
            "Incepe procesul 0 (`root`). Incep un task care dureaza 4 secunde.\n",
            "Procesul 0 (`root`) a terminat job-ul de 4 secunde.\n",
            "Procesul 0 va dura 1.92 secunde.\n",
            "Procesul 2 a terminat de asteptat.Procesul 4 a terminat de asteptat.\n",
            "Procesul 1 a terminat de asteptat.Procesul 3 a terminat de asteptat.\n",
            "Procesul 2 va dura 10.00 secunde.\n",
            "\n",
            "Procesul 1 va dura 3.01 secunde.\n",
            "Procesul 3 va dura 0.66 secunde.Procesul 4 va dura 9.93 secunde.\n",
            "\n",
            "\n",
            "Procesul 3 si-a terminat treaba.\n",
            "Procesul 3 asteapta la a 2-a bariera.\n",
            "Procesul 0 si-a terminat treaba.\n",
            "Procesul 0 asteapta la a 2-a bariera.\n",
            "Procesul 1 si-a terminat treaba.\n",
            "Procesul 1 asteapta la a 2-a bariera.\n",
            "Procesul 4 si-a terminat treaba.\n",
            "Procesul 4 asteapta la a 2-a bariera.\n",
            "Procesul 2 si-a terminat treaba.\n",
            "Procesul 2 asteapta la a 2-a bariera.\n",
            "========================================================\n",
            "===    Procesul 0: Toate procesele s-au terminat.    ===\n",
            "========================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Broadcast\n",
        "\n",
        "Scrieti un program paralel in care procesul zero (`root`) foloseste rutina de broadcast pentru a trimite un dictionar cu doua chei `key1` : `[3, 24.62, 9+4j]`, `key2` : ( `fmi`, `unibuc`) catre toate procesele.\n",
        "\n",
        "Apoi scrieti o versiune care trimite un vector in loc de dictionar."
      ],
      "metadata": {
        "id": "LPCKiTHvHYUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "id": "pAR_F6YQHykV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426bbb1f-5282-4714-bd95-f6c490a66e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m460.8/466.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4442045 sha256=1c41bc8c425b2a6fd7f86a6b8477a7b1e50167c5a1ca109960c8c917921f9fc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile broadcast_task_1.py\n",
        "\n",
        "# Folosesc libraria Message Passing Interface (MPI) din python.\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size() # Numarul total de procese\n",
        "rank = comm.Get_rank() # ID-ul unui proces, de la 0 la size-1\n",
        "\n",
        "\n",
        "# Dictionarul e definit pentru procesul 0, care il trimite si celorlalte\n",
        "# procese.\n",
        "if rank == 0:\n",
        "    dictionar = {\n",
        "        'key1': [3, 24.62, 9+4j],\n",
        "        'key2': ('fmi', 'unibuc')\n",
        "    }\n",
        "    print(f\"Procesul 0 (root) transmite dictionarul:\\n{dictionar}\\n\")\n",
        "else:\n",
        "    # Pentru celelalte procese initial e null\n",
        "    dictionar = None\n",
        "\n",
        "# root = 0 spune practic ca procesul 0 e cel care face send.\n",
        "received_dict = comm.bcast(dictionar, root=0)\n",
        "\n",
        "print(f\"Procesul {rank} a primit dictionarul:\\n{received_dict}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjicQqVKIMVZ",
        "outputId": "7f781f9e-ee33-4e9d-ba01-5bbe99cceb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting broadcast_task_1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec -n 4 --allow-run-as-root --oversubscribe python broadcast_task_1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgWXC3L8I-Zy",
        "outputId": "d09c0ac9-69b3-4a02-996f-acda51c764a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesul 0 (root) transmite dictionarul:\n",
            "{'key1': [3, 24.62, (9+4j)], 'key2': ('fmi', 'unibuc')}\n",
            "\n",
            "Procesul 0 a primit dictionarul:\n",
            "{'key1': [3, 24.62, (9+4j)], 'key2': ('fmi', 'unibuc')}\n",
            "Procesul 2 a primit dictionarul:\n",
            "{'key1': [3, 24.62, (9+4j)], 'key2': ('fmi', 'unibuc')}\n",
            "Procesul 1 a primit dictionarul:\n",
            "{'key1': [3, 24.62, (9+4j)], 'key2': ('fmi', 'unibuc')}\n",
            "Procesul 3 a primit dictionarul:\n",
            "{'key1': [3, 24.62, (9+4j)], 'key2': ('fmi', 'unibuc')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile broadcast_task_2.py\n",
        "\n",
        "# Folosesc libraria Message Passing Interface (MPI) din python.\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size() # Numarul total de procese\n",
        "rank = comm.Get_rank() # ID-ul unui proces, de la 0 la size-1\n",
        "\n",
        "\n",
        "# Vectorul e definit pentru procesul 0, care il trimite si celorlalte procese.\n",
        "if rank == 0:\n",
        "    vector = [1, 2, 3]\n",
        "    print(f\"Procesul 0 (root) transmite vectorul:\\n{vector}\\n\")\n",
        "else:\n",
        "    # Pentru celelalte procese initial e null\n",
        "    vector = None\n",
        "\n",
        "# root = 0 spune practic ca procesul 0 e cel care face send.\n",
        "received_vect = comm.bcast(vector, root=0)\n",
        "\n",
        "print(f\"Procesul {rank} a primit vector:\\n{received_vect}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KAhvuJbGMxx",
        "outputId": "a98edaeb-ac4e-4e21-ff20-3b34dba6e8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting broadcast_task_2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec -n 4 --allow-run-as-root --oversubscribe python broadcast_task_2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe-Meq2KGiYc",
        "outputId": "2dc50f76-90ac-4f8b-eb80-dac1b9ddd279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesul 0 (root) transmite vectorul:\n",
            "[1, 2, 3]\n",
            "\n",
            "Procesul 0 a primit vector:\n",
            "[1, 2, 3]\n",
            "Procesul 1 a primit vector:\n",
            "[1, 2, 3]\n",
            "Procesul 2 a primit vector:\n",
            "[1, 2, 3]\n",
            "Procesul 3 a primit vector:\n",
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Scatter\n",
        "\n",
        "Procesul zero (`root`) creaza o matrice $p \\times p$ de numere reale (nr de linii egal cu nr de procesoare din comunicator). Scrieti un program paralel in care procesul zero foloseste rutina de distributie personalizata (`scatter`) pentru a trimite fiecarui procesor cu id `rank` din comunicator un mesaj compus din coloana cu indexul rank si respectiv linia cu indexul `rank`. Afisati pentru verificare."
      ],
      "metadata": {
        "id": "6PYvMBACJ4PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scatter_task.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size() # Numarul total de procese\n",
        "rank = comm.Get_rank() # ID-ul unui proces, de la 0 la size-1\n",
        "\n",
        "# Numarul de linii = nr. de procesoare\n",
        "p = size\n",
        "\n",
        "if rank == 0:\n",
        "    # In procesul se creeaza matricea de p*p.\n",
        "    matrix = np.arange(p * p, dtype=np.float64).reshape(p, p)\n",
        "    print(f\"Procesul 0 a creat matrice de {p}x{p}:\\n{matrix}\\n\")\n",
        "\n",
        "    for i in range(p):\n",
        "        row_to_send = np.copy(matrix[i, :])\n",
        "        col_to_send = np.copy(matrix[:, i])\n",
        "\n",
        "        message = (row_to_send, col_to_send)\n",
        "\n",
        "        print(f\"Procesul 0 trimite linia si coloana {i} to procesului {i}.\")\n",
        "        comm.send(message, dest=i)\n",
        "\n",
        "else:\n",
        "    # Primesc mesajul de la procesul 0\n",
        "    received_message = comm.recv(source=0)\n",
        "    received_row, received_col = received_message\n",
        "\n",
        "    # Afisez pentru verificare\n",
        "    print(f\"Procesul {rank} a primi:\")\n",
        "    print(f\"   * Linia {rank}: {received_row}\")\n",
        "    print(f\"   * Coloana {rank}: {received_col}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfN4KmkUKUJb",
        "outputId": "061bcb34-3578-437e-d791-6e750a173372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scatter_task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec -n 4 --allow-run-as-root --oversubscribe python scatter_task.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3FCgg7dKswL",
        "outputId": "302302e4-c53e-4d24-a5d2-6cb37e3ed06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/scatter_task.py\", line 2, in <module>\n",
            "    from mpi4py import MPI\n",
            "ModuleNotFoundError: No module named 'mpi4py'\n",
            "--------------------------------------------------------------------------\n",
            "Primary job  terminated normally, but 1 process returned\n",
            "a non-zero exit code. Per user-direction, the job has been aborted.\n",
            "--------------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/scatter_task.py\", line 2, in <module>\n",
            "    from mpi4py import MPI\n",
            "ModuleNotFoundError: No module named 'mpi4py'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/scatter_task.py\", line 2, in <module>\n",
            "    from mpi4py import MPI\n",
            "ModuleNotFoundError: No module named 'mpi4py'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/scatter_task.py\", line 2, in <module>\n",
            "    from mpi4py import MPI\n",
            "ModuleNotFoundError: No module named 'mpi4py'\n",
            "--------------------------------------------------------------------------\n",
            "mpiexec detected that one or more processes exited with non-zero status, thus causing\n",
            "the job to be terminated. The first process to do so was:\n",
            "\n",
            "  Process name: [[6594,1],0]\n",
            "  Exit code:    1\n",
            "--------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Inmultire matrice-vector pe inel.\n",
        "\n",
        "Presupunem ca nodurile din comunicator (in numar de $p$) sunt distribuite pe o topologie de tip inel. Procesul zero (`root`) stocheaza o matricile $A \\in \\mathbb{R}^{n \\times n}$ si $x \\in \\mathbb{R}^n$.\n",
        "\n",
        "*  Realizati o distributie echilibrata a tablourilor astfel incat nodul $i$ sa stocheze (bloc-)coloana $i$ din $A$ si (bloc-)componenta $i$ din $x$.\n",
        "\n",
        "* Scrieti o procedura care realizeaza inmultirea eficienta matrice-vector in paralel. In urma unei difuzari generale pe inel, rezultatul va fi acumulat in fiecare nod din comunicator."
      ],
      "metadata": {
        "id": "7g1AYY74LTaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matvec_ring.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size() # Numarul total de procese\n",
        "rank = comm.Get_rank() # ID-ul unui proces, de la 0 la size-1\n",
        "\n",
        "# Am luat, de exemplu n = 4 (si atunci numarul de procese va trebuie sa fie 1,\n",
        "# 2 sau 4).\n",
        "n = 4\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"Numar procese (p): {size}, Dimensiune matrice (n): {n}\\n\")\n",
        "\n",
        "    # Generez aleator matricea si vectorul\n",
        "    A = np.random.randint(5, size = (n, n))\n",
        "    x = np.random.randint(5, size = n)\n",
        "\n",
        "    print(f\"Matricea A:\\n{A}\")\n",
        "    print(f\"Vectorul x:\\n{x}\\n\")\n",
        "\n",
        "    # Calculul \"normal\" pentru verificare\n",
        "    expected_y = np.dot(A, x)\n",
        "\n",
        "    d = n // size\n",
        "\n",
        "    # Root pregateste blocurile pentru a le imparti celorlalte procese\n",
        "    A_cols = [A[:, i:i + d].copy() for i in range(0, n, d)]\n",
        "    x_parts = [x[i:i + d] for i in range(0, n, d)]\n",
        "\n",
        "    # Pastreaza primul bloc pentru el\n",
        "    local_A_block = A_cols[0]\n",
        "    local_x_block = x_parts[0]\n",
        "\n",
        "    # Trimite restul blocurilor vecinului din dreapta\n",
        "    if size > 1:\n",
        "        comm.send({'A': A_cols[1:], 'x': x_parts[1:]}, dest=1)\n",
        "\n",
        "else:\n",
        "    # Fiecare proces primeste de la vecinul din stanga\n",
        "    received_data = comm.recv(source=rank - 1)\n",
        "\n",
        "    # Pastreaza primul bloc din ce a primit\n",
        "    local_A_block = received_data['A'][0]\n",
        "    local_x_block = received_data['x'][0]\n",
        "\n",
        "    # Daca nu ești ultimul proces, trimite restul mai departe la dreapta\n",
        "    if rank < size - 1:\n",
        "        remaining_data = {'A': received_data['A'][1:], 'x': received_data['x'][1:]}\n",
        "        comm.send(remaining_data, dest=rank + 1)\n",
        "\n",
        "\n",
        "# Fiecare proces 'i' calculează \"componenta: locala\n",
        "partial_y = np.dot(local_A_block, local_x_block)\n",
        "\n",
        "# Adunarea rezultatelor (\"Reduce\" manual pe inel)\n",
        "if rank > 0:\n",
        "    accumulated_sum = partial_y.copy()\n",
        "\n",
        "    # Daca nu ești ultimul proces (p-1), primește suma de la dreapta\n",
        "    if rank < size - 1:\n",
        "        sum_from_right = comm.recv(source=rank + 1)\n",
        "        accumulated_sum += sum_from_right\n",
        "\n",
        "    # Trimite suma acumulata la stanga\n",
        "    comm.send(accumulated_sum, dest=rank - 1)\n",
        "\n",
        "else:\n",
        "    final_result = partial_y.copy()\n",
        "\n",
        "    if size > 1:\n",
        "        sum_from_others = comm.recv(source=1)\n",
        "        final_result += sum_from_others\n",
        "\n",
        "# Verificare\n",
        "if rank == 0:\n",
        "    print(f\"Rezultat final (paralel): {final_result}\")\n",
        "    print(f\"Rezultatul așteptat: {expected_y}\\n\")"
      ],
      "metadata": {
        "id": "Fl8zApc4GCXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486fa53c-0dfc-4d8a-af6b-b3e45d37cd2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matvec_ring.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec -n 4 --allow-run-as-root --oversubscribe python matvec_ring.py"
      ],
      "metadata": {
        "id": "0Cuwm4UPOONs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}